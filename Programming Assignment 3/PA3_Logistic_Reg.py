# -*- coding: utf-8 -*-
"""Programming_Assignment_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TDNsY2o_0A8ue8NjMvPGm7LzDY8oJIxS
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import glob
import collections
from collections import Counter
from scipy.special import expit

# !unzip -q "/content/drive/My Drive/Machine Learning Fall 2020/Programming_Assignment_3.zip" -d "/content/drive/My Drive/Machine Learning Fall 2020/Programming Assignment 3"

!gdown --id 1n0j5zbMXMVnLPUqadFNcCxiZRu0ChoXL
!unzip -q "Programming_Assignment_3.zip" -d ""

test = pd.DataFrame()
train = pd.DataFrame()
label_list = []
content = []

def file_reading(datatype,directory):
#   files = glob.glob('Dataset/'+datatype+'/'+directory+'/*.txt',  recursive = True)
    files = glob.glob('Dataset/'+datatype+'/'+directory+'/*.txt',  recursive = True)

    print("Files are :",len(files))
  # return
    if directory == 'neg':
        label = 0
    else:
        label = 1

    print("Sentiment is: ",label)
    for file in files:
        f = open(file,'r')
        content.append(f.read())
        label_list.append(label)
    print('Content:  ',content[0])

  
  
  
  # if datatype=='test':
  #   # # test['Review'] = content
  #   # test['bias'] = 1
  #   # test['Postive count'] = 0
  #   # test['Negative count'] = 0
  #   # # test['Label'] = label_list
  # else:
    # train['Review'] = content
    # train['bias'] = 1
    # train['Postive count'] = 0
    # train['Negative count'] = 0
    # train['Label'] = label_list

file_reading('test','neg')
file_reading('test','pos')

test['Review'] = content
test['bias'] = 1
test['Postive count'] = 0
test['Negative count'] = 0
test['Label'] = label_list

test[test['Label']==1]

label_list = []
content = []
file_reading('train','neg')
file_reading('train','pos')
train['Review'] = content
train['bias'] = 1
train['Postive count'] = 0
train['Negative count'] = 0
train['Label'] = label_list

# train['Review'] = content
# train['bias'] = 1
# train['Postive count'] = 0
# train['Negative count'] = 0
# train['Label'] = label_list

train[train['Label'] == 1]

# len(content)
# content[0]

# print(label_list)
# print(len(label_list))

train.head()

train[train['Label']==0]

def preprocessing(data):
    data['Review'] = data['Review'].str.lower()
    data['without_stopwords'] = data['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
    data['Review'] = data['without_stopwords']
    data.drop(columns='without_stopwords',inplace = True)
    data['Review'] = data['Review'].str.replace('[^\w\s\d]','')
    return data

f = open('Dataset/stop_words.txt','r')
stop_words = f.read()

pos = open('Dataset/positive_words.txt','r')
pos_words = pos.read()

nega = open('Dataset/negative_words.txt','r',encoding='latin-1')
# negative_words = ""

negative_words = nega.read()
# print('Try',negative_words)
# try:
#   neg_words = negative_words.decode('Latin')
# except:
#   print("error")


train = preprocessing(train)
test = preprocessing(test)

train.head()

test.head()



def count_update(data,datatype):
    index = 0
    for i in data['Review']:
        i=i.lower()
        neg_count = 0 
        pos_count = 0 
        word_count = Counter(i.split())
        # print(word_count)
        for k,v in word_count.items():
            if k in pos_words:
                pos_count = pos_count + v
            elif k in negative_words:
                neg_count = neg_count + v
            # print("Neg ", k)
        if datatype=="test":
            test['Negative count'][index] = neg_count
            test['Postive count'][index] = pos_count
            index = index + 1
            data = test

        else:
            train['Negative count'][index] = neg_count
            train['Postive count'][index] = pos_count
            index = index + 1
            data = train

    return data

train = count_update(train,"train")
test = count_update(test,"test")

test.head()

X_train = np.asarray(train[["bias","Postive count","Negative count"]])
y_train = np.asarray(train['Label'])
X_test = np.asarray(test[["bias","Postive count","Negative count"]])
y_test = np.asarray(test['Label'])



"""# Part 1

**Sigmoid function**
"""

def predict_old(data,list_theta):
    list_theta = np.transpose(np.array(list_theta))
    hx = list_theta*data
    # h_x = hx.sum()

    hx_final = 1/(1+np.exp(-1*hx))
    h_xfinal = expit(data @ list_theta)


    return hx_final,h_xfinal

# values = predict(X_train,thetas)
# values

thetas = np.zeros(len(X_train[0]))
# val = predict(X_train[1],thetas)
# val

"""**Cost Function**"""

epsilon = 1e-5

def entropy_loss_old(X,Y,list_theta):
    m = Y.size
    J = 0

    temp = 0
    _,hx = predict(X,list_theta)
    for index in range(m):

        value = (Y[index]*np.log(hx[index] + epsilon)) + ((1-Y[index])*(np.log(1-hx[index] + epsilon)))
        temp = temp + value
    J = -1*(1/m)*temp

    return J

# temp_X = X_train[-5:]
# temp_y = y_train[-5:]

# # [0] in y_train

# thetas = np.zeros(len(X_train[0]))
# loss = entropy_loss(X_train,y_train,thetas)
# loss



def LR_gradientDescent_old(X,Y,alpha,n_epoch):
    m = Y.size  
    J = list()  
    thetas = np.zeros(len(X[0]))
    epoch_ls = []
    epoch_j = []
    for epoch in range(n_epoch):
        temp_thetas = []
        temp_1 = 0
        _,hx = predict(X,thetas)
        for j in range(len(thetas)):
            for i in range(m):
                value = (hx[i] - Y[i])*X[i][j]
                temp_1 = temp_1 + value
                temp_1 = thetas[j] - alpha*(1/m)*temp_1
            temp_thetas.append(temp_1)


    print("temp thetas : ",entropy_loss(X, Y, thetas))
    for i in range(0,len(thetas)):
        thetas[i] = temp_thetas[i]


    J.append(entropy_loss(X, Y, thetas))

    if (epoch % 100 == 0):
        epoch_ls.append(epoch)
        epoch_j.append(J[-1])
        print('Cost at Epoch ', epoch,' ',J[-1])
    return thetas,J,epoch_ls,epoch_j

# n_epoch = 500
# alpha = 0.01

# finalthetas, J,epoch_ls,epoch_j = LR_gradientDescent(X_train,y_train,alpha,n_epoch)
# # print('Value of thetas', thetas)
# print('Value of Cost ftn', J[-1])

# def Cross_entropy_loss(X,Theta,Y_TrainData):
#     h_theta= Sigmoid(X,Theta) 
#     Y_TrainData=Y_TrainData.reshape(Y_TrainData.shape[0],1)
#     #Using log2 base for cross entropy loss
#     return (-1/X.shape[0]) * np.sum(Y_TrainData*np.log2(h_theta  +1e-10)+  (1-Y_TrainData)*np.log2(1-h_theta +1e-10))

def predict(data,list_theta):
  #list_theta = np.transpose(np.array(list_theta))
  hx =data.dot(list_theta) #list_theta*data
  # h_x = hx.sum()
  
  hx_final = 1/(1+np.exp(-1*hx))
  h_xfinal = expit(data @ list_theta)


  return hx_final,h_xfinal

epsilon = 1e-10

def entropy_loss(X,Y,list_theta):
  m = Y.size
  J = 0

  temp = 0
  _,hx = predict(X,list_theta)
  for index in range(m):
    
    value = (Y[index]*np.log(hx[index] + epsilon)) + ((1-Y[index])*(np.log(1-hx[index] + epsilon)))
    temp = temp + value
  J = -1*(1/m)*temp

  return J[0]

def LR_gradientDescent(X,Y,alpha,n_epoch):
    m = Y.size  
    J = list()  
    thetas = np.zeros(len(X[0])).reshape(-1,1)
    print(thetas.shape)
    epoch_ls = []
    epoch_j = []
    for epoch in range(n_epoch):

        # cost_val=entropy_loss(X, Y, thetas)
        _,hx = predict(X,thetas)

        hx=(hx.reshape(-1,1),thetas)[0]

        thetas-= (alpha/X.shape[0] )*  (   ((  hx -  Y ).T) .dot(X) ).T


        J.append(entropy_loss(X, Y, thetas))
        if (epoch % 10 == 0):
          epoch_ls.append(epoch)
          epoch_j.append(J[-1])
        if (epoch%100 == 0):
          print('Cost at Epoch ', epoch,' ',J[-1])
    # temp_thetas = []
    # temp_1 = 0
    # _,hx = predict(X,thetas)
    # for j in range(len(thetas)):
    #   for i in range(m):
    #     value = (hx[i] - Y[i])*X[i][j]
    #     temp_1 = temp_1 + value
    #   temp_1 = thetas[j] - alpha*(1/m)*temp_1
    #   temp_thetas.append(temp_1)

    
    # print("temp thetas : ",entropy_loss(X, Y, thetas))
    # for i in range(0,len(thetas)):
    #   thetas[i] = temp_thetas[i]
     

    J.append(entropy_loss(X, Y, thetas))
    #print("cost ",entropy_loss(X, Y, thetas))
    # if (epoch % 100 == 0):
    #     epoch_ls.append(epoch)
    #     epoch_j.append(J[-1])
    #     print('Cost at Epoch ', epoch,' ',J[-1])
    return thetas,J,epoch_ls,epoch_j

X_train = np.asarray(train[["bias","Postive count","Negative count"]])
y_train = np.asarray(train['Label'])
X_test = np.asarray(test[["bias","Postive count","Negative count"]])
y_test = np.asarray(test['Label'])
y_train=(y_train.reshape(-1,1))

n_epoch = 1500
alpha = 0.001

finalthetas, J,epoch_ls,epoch_j = LR_gradientDescent(X_train,y_train,alpha,n_epoch)
# print('Value of thetas', thetas)
print('Value of Cost ftn', J[-1])



"""**Making prediction**"""

def making_pred(X,Y,thetas):
    m = len(Y)
    predicted_labels = []
    _,hx = predict(X,thetas)
    for i in range(m):
    # _,hx = predict(X[i],thetas)
    # print(hx)

      if hx[i] < 0.5:
        predicted_labels.append(0)
      else:
        predicted_labels.append(1)
    return predicted_labels

# temp_Xtest = X_test[-5:]
# temp_ytest = y_test[-5:]

ls_labels = making_pred(X_test,y_test,finalthetas)
print(ls_labels)

"""**Evaluation**"""

def classification_accuracy(actual, predicted):
    total = len(actual)
    correct = 0
    for i in range(total):
        if actual[i] == predicted[i]:
            correct = correct + 1
  # query = np.where(actual == predicted,1,0)
  # occurrence_counts = Counter(query)
  # correct = occurrence_counts[1]
    accuracy = (correct/total) * 100

    return accuracy

# ls_labels = np.asarray(ls_labels)
y_test = list(test['Label'])
# y_test
acc = classification_accuracy(y_test,ls_labels)
print("The accuracy of the classifier is: ",acc,"%")

def confusion_matrix(actual,predicted):
  tp = 0
  fp = 0
  tn = 0
  fn = 0
  for i in range(len(actual)):
    if actual[i]==1:
      if actual[i] == predicted[i]:
        tp = tp + 1
      else:
        fn = fn +1
    else:
      if actual[i] == predicted[i]:
        tn = tn + 1
      else:
        fp = fp + 1
  return (tp,fp,tn,fn)

(tp,fp,tn,fn) = confusion_matrix(y_test,ls_labels)
class_matrix = [[tp,fp],[fn,tn]]
print("Classification Matrix: ")
print( class_matrix)

"""**Plots**"""

epoch_ls.append(n_epoch)
epoch_j.append(J[-1])
# epoch_j



finalthetas

# plt.figure(figsize=(20,10))
ax = plt.gca()

ax.plot(epoch_ls,epoch_j)
plt.xlabel('Epochs')
plt.ylabel('training Loss')
plt.title('Epochs vs Training Loss')

plt.show()

"""# Part 2"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

clf = LogisticRegression(random_state=0).fit(X_train, y_train)
predicted = clf.predict(X_test)

accuracy = accuracy_score(y_test,predicted)
conf_mat = confusion_matrix(y_test,predicted)

print("Accuracy is: ",accuracy)
print("Confusion Matrix:\n",conf_mat)

